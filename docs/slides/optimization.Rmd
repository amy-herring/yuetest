---
title: "Fitting linear models in R"
author: "Yue Jiang"
date: "Duke University"
header-includes: \usepackage{bm}
output:
  xaringan::moon_reader:
  mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
css: "sta440-slides.css"
logo: img/sta199-sticker-icon.png
lib_dir: libs/font-awesome
nature:
  highlightStyle: github
highlightLines: true
countIncrementalSlides: false
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")

options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
	fig.align = "center",
	fig.height = 3.75,
	fig.width = 6.25,
	message = FALSE,
	warning = FALSE
)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

### Estimating bike crashes in NC counties

```{r, eval=TRUE, echo=FALSE, warning = F, message = F, out.width = "100%"}
library(knitr)
include_graphics("img/bike_crash.jpg")
```
<!-- .center[Image credit: Petar Milošević, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0), via Wikimedia Commons] -->

---

### Data

```{r, echo = F}
bike <- read_csv("data/bikecrash.csv")
bike %>% 
  select(county, crashes)
```
Suppose we thought these crashes came from a Poisson distribution. 
.question[
How might we estimate the parameter of that Poisson distribution, given our
observed data?
]

---
  
<!-- ### Maximum likelihood estimation -->

<!-- If $Y \sim Pois(\lambda)$, then -->

<!-- \begin{align*} -->
<!--   P(Y = y) = \frac{\lambda^y e^{-\lambda}}{y!} -->
<!-- \end{align*} -->

<!-- .question[ -->
<!-- If $Y_1, Y_2, \cdots, Y_n$ are each i.i.d. distributed with $Pois(\lambda)$, -->
<!-- then what is the **Maximum Likelihood Estimate** (MLE) of $\lambda$? -->

### Maximum likelihood estimation

We can maximize the .vocab[likelihood function]. Assuming the observations are
i.i.d., in general we have:

\begin{align*}
\mathcal{L}(\lambda | Y) &= f(y_1, y_2, \cdots, y_n | \lambda)\\
&= f(y_1 | \lambda)f(y_2 | \lambda) \cdots f(y_n | \lambda)\\
&= \prod\limits^{n}_{i = 1} f(y_i | \lambda).
\end{align*}

The likelihood function is the probability of "seeing our observed data," 
**given** a value of $\lambda$. Do not get $f(y_i | \lambda)$ confused with 
$f(\lambda | y_i)$!

---

### Maximum likelihood estimation

For our Poisson example, we thus have:

\begin{align*}
\mathcal{L}(\lambda | Y) &= \prod\limits^{n}_{i = 1} f(y_i | \lambda)\\
&= \prod\limits_{i = 1}^n \frac{\lambda^y_i e^{-\lambda}}{y_i!}\\
\log \mathcal{L}(\lambda | Y) &= \sum_{i = 1}^n \left( y_i\log \lambda - \lambda - \log y_i! \right)\\
&= \log \lambda \sum_{i = 1}^n y_i - n\lambda - \sum_{i = 1}^n \log y_i!
\end{align*}

.question[
Why do we maximize the log-likelihood function here?
]

---

### Maximum likelihood estimation

Setting the .vocab[score function] equal to 0:

\begin{align*}
\frac{\partial}{\partial\lambda}\log \mathcal{L}(\lambda | Y) = \frac{1}{\lambda}\sum_{i = 1}^n y_i - n &\stackrel{set}{=} 0\\
\implies \hat{\lambda} = \frac{1}{n}\sum_{i = 1}^n y_i,
\end{align*}

as expected. Next, let's verify that $\hat{\lambda}$ is indeed a maximum:

\begin{align*}
\frac{\partial^2}{\partial\lambda^2}\log \mathcal{L}(\lambda | Y) &= -\frac{1}{\lambda^2}\sum_{i = 1}^n y_i - n\\
&< 0.
\end{align*}

---

### Can we do better?

```{r echo = F}
bike
```

We might expect that more populous, more urban counties might have more crashes.
There might also be a relationship with traffic volume. 

.quesiton[
Can we incorporate this additional information while accounting for potential
confounding?
]

---

### Poisson regression

\begin{align*}
\log\large(\underbrace{E(Y | \mathbf{X}}_{\lambda})\large) = \beta_0 + \mathbf{X}^T\boldsymbol\beta
\end{align*}

.vocab[Generalized linear model] often used for count (or rate) data
- Assumes outcome has Poisson distribution
- Canonical link: log of conditional expectation of response has linear 
relationship with predictors

.question[
Can we differentiate the (log) likelihood function, set it equal to zero, and
solve for the MLEs for $\boldsymbol\beta = (\beta_0, \beta_1, \cdots, \beta_p)$
as before?
]

---

### Poisson regression

\begin{align*}
\log \mathcal{L}&= \sum_{i = 1}^n \left( y_i\log \lambda - \lambda - \log y_i! \right)\\
&= \sum_{i = 1}^n y_i\mathbf{X}_i\boldsymbol\beta - e^{\mathbf{X}_i\boldsymbol\beta} - \log y_i!
\end{align*}

We would like to solve the equations

\begin{align*}
\left(\frac{\partial \log \mathcal{L}}{\partial \beta_j}\right) \stackrel{set}{=} \mathbf{0},
\end{align*}

but there is no closed-form solution, as this is a transcendental equation in 
the parameters of interest.

.question[
How might we solve these equations numerically?
]

---

### A one-dimensional problem

Suppose you're trying to find the maximum of the following function:

\begin{align*}
f(x) = \frac{x + \log(x)}{2^x}
\end{align*}

Let's try differentiating, setting equal to 0, and solving:

\begin{align*}
\frac{d}{dx}f(x) = 2^{-x}\left(1 + \frac{1}{x} - \log(2)(x + \log(x)) \right).
\end{align*}

We run into a similar problem: we cannot algebraically solve for the root of 
this equation.

---

### A one-dimensional problem

```{r, echo = F, fig.height = 5.5, fig.width = 10}
func <- function(x){
  (x + log(x))/(2^x)
}

func2 <- function(x){
  2^(-1 * x) * (1 + 1/x - log(2) * (x + log(x)))
}

par(mfrow = c(1, 2))
plot(func, from = 0, to = 5, ylab = "", xlab = "", xlim = c(0, 4), main = "Original function")
plot(func2, from = 0, to = 5, ylab = "", xlab = "", xlim = c(0, 4), ylim = c(-0.5, 5), main = "First derivative")
abline(h = 0, col = "gray")
```

.question[
It looks lke the maximum is a bit shy of 2 (trust me on this one, it's a global
maximum). How might we find where it is?
]

---

### A one-dimensional problem

```{r, eval=TRUE, echo=FALSE, warning = F, message = F, out.width = "80%"}
include_graphics("img/newton.png")
```

---

### A one-dimensional problem

.vocab[Newton-Raphson] algorithm for root finding is based on second-order Taylor 
approximation around true root:

- Start with initial guess $\theta^{(0)}$
- Iterate $\theta^{(t + 1)} = \theta^{(t)} - \frac{f^\prime(\theta^{(t)})}{f^{\prime\prime} (\theta^{(t)})}$
- Stop when convergence criterion is satisfied

Although it requires explicit forms of first two derivatives, the convergence
speed is quite fast. 

There are some necessary conditions for convergence, but this is beyond the
scope of STA 440. Many likelihood functions you are likely to encounter (e.g., 
GLMs with canonical link) will in fact converge from any starting value.

---

### A one-dimensional problem

```{r, echo = F, fig.height = 7, fig.width = 11}
library(animation)
nr  <- function (FUN = function(x) x^2 - 4, init = 10, rg = c(-1, 10), 
    tol = 0.001, interact = FALSE, col.lp = c("blue", "red", 
        "red"), main, xlab, ylab, ...) 
{
    if (interact) {
        curve(FUN, min(rg), max(rg), xlab = "x", ylab = eval(substitute(expression(f(x) == 
            y), list(y = body(FUN)))), main = "Locate the starting point")
        init = unlist(locator(1))[1]
    }
    i = 1
    nms = names(formals(FUN))
    grad = deriv(as.expression(body(FUN)), nms, function.arg = TRUE)
    x = c(init, init - FUN(init)/attr(grad(init), "gradient"))
    gap = FUN(x[2])
    if (missing(xlab)) 
        xlab = nms
    if (missing(ylab)) 
        ylab = eval(substitute(expression(f(x) == y), list(y = body(FUN))))
    if (missing(main)) 
        main = eval(substitute(expression("Root-finding by Newton-Raphson Method:" ~ 
            y == 0), list(y = body(FUN))))
    nmax = ani.options("nmax")
    while (abs(gap) > tol & i <= nmax & !is.na(x[i + 1])) {
        dev.hold()
        curve(FUN, min(rg), max(rg), main = main, xlab = xlab, 
            ylab = ylab, ...)
        abline(h = 0, col = "gray")
        segments(x[1:i], rep(0, i), x[1:i], FUN(x[1:i]), col = col.lp[1])
        segments(x[1:i], FUN(x[1:i]), x[2:(i + 1)], rep(0, i), 
            col = col.lp[2])
        points(x, rep(0, i + 1), col = col.lp[3])
        points(x[1:i], FUN(x[1:i]), col = col.lp[3])
        mtext(paste("t = ", i, ": Current root: ", round(x[i + 1], 3), sep = ""), 3)
        gap = FUN(x[i + 1])
        x = c(x, x[i + 1] - FUN(x[i + 1])/attr(grad(x[i + 1]), 
            "gradient"))
        ani.pause()
        i = i + 1
    }
    rtx = par("usr")[4]
#    arrows(x[i], rtx, x[i], 0, col = "gray")
    invisible(list(root = x[i], value = gap, iter = i - 1))
}

par(mfrow = c(2, 3))
nr(function(x) 2^(-1 * x) * (1 + 1/x - log(2) * (x + log(x))), 0.3, c(0, 4),
   main = "", xlab = "", ylab = "", ylim = c(-0.5, 5))
```

---

### A one-dimensional problem

\begin{align*}
f(x) &= \frac{x + \log(x)}{2^x} \\
\frac{d}{dx}f(x) &= 2^{-x}\left(1 + \frac{1}{x} - \log(2)(x + \log(x)) \right).
\end{align*}

```{r}
testing <- function(x){
  2^(-1 * x) * (1 + 1/x - log(2) * (x + log(x)))
}

testing(1.729)
```

That's pretty good (only six steps from starting guess of 0.3)!

---

### Newton-Raphson in higher dimensions

.vocab[Score vector] and .vocab[Hessian] for $\log \mathcal{L}(\boldsymbol\theta | \mathbf{X})$ with $\boldsymbol\theta = (\theta_1, \cdots, \theta_p)^T$:

$$\nabla \log \mathcal{L} = \begin{pmatrix}
\frac{\partial \log \mathcal{L}}{\partial \boldsymbol\theta_1}\\
\vdots\\
\frac{\partial \log \mathcal{L}}{\partial \boldsymbol\theta_p}
\end{pmatrix}$$
$$\nabla^2 \log \mathcal{L} = \begin{pmatrix}
\frac{\partial^2 \log\mathcal{L}}{\partial \theta_1^2} & \frac{\partial^2 \log\mathcal{L}}{\partial \theta_1 \theta_2} & \cdots & \frac{\partial^2 \log\mathcal{L}}{\partial \theta_1\theta_p}\\
\frac{\partial^2 \log\mathcal{L}}{\partial \theta_2\theta_1} & \frac{\partial^2 \log\mathcal{L}}{\partial \theta_2^2} & \cdots & \frac{\partial^2 \log\mathcal{L}}{\partial \theta_2\theta_p} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 \log\mathcal{L}}{\partial \theta_p\theta_1} & \frac{\partial^2 \log\mathcal{L}}{\partial \theta_p\theta_2} & \cdots & \frac{\partial^2 \log\mathcal{L}}{\partial \theta_p^2}
\end{pmatrix}$$
 
---

### Newton-Raphson in higher dimensions

We can modify the Newton-Raphson algorithm for higher dimensions: 

- Start with initial guess $\boldsymbol\theta ^{(0)}$
- Iterate $\boldsymbol\theta^{(t + 1)} = \boldsymbol\theta^{(t)} - \left(\nabla^2\log\mathcal{L}(\boldsymbol\theta^{(t)} | \mathbf{X}) \right)^{-1} \left( \nabla \log\mathcal{L}(\boldsymbol\theta^{(t)} | \mathbf{X}) \right)$
- Stop when convergence criterion is satisfied

Under certain conditions, a global maximum exists; this again is guaranteed for 
many common applications. 

Computing the Hessian can be computationally demanding (and annoying), but there 
are ways around it in practice. 

---

### Poisson regression

\begin{align*}
\log \mathcal{L}&= \sum_{i = 1}^n y_i\mathbf{X}_i\boldsymbol\beta - e^{\mathbf{X}_i\boldsymbol\beta} - \log y_i!\\
\nabla \log \mathcal{L}&= \sum_{i = 1}^n \left(y_i - e^{\mathbf{X}_i\boldsymbol\beta}\right)\mathbf{X}_i^T\\
\nabla^2 \log \mathcal{L} &= -\sum_{i = 1}^n e^{\mathbf{X}_i\boldsymbol\beta}\mathbf{X}_i\mathbf{X}_i^T
\end{align*}

Newton-Raphson update steps for Poisson regression: 

\begin{align*}
\boldsymbol\beta^{(t+1)} &= \boldsymbol\beta^{(t)} - \left(-\sum_{i = 1}^n e^{\mathbf{X}_i\boldsymbol\beta}\mathbf{X}_i\mathbf{X}_i^T \right)^{-1}\left(\sum_{i = 1}^n \left(y_i - e^{\mathbf{X}_i\boldsymbol\beta}\right)\mathbf{X}_i^T \right)
\end{align*}

---

### Back to bike crashes

```{r echo = F}
bike
```

- `pop`: county population
- `med_hh_income`: median household income in thousands
- `traffic_vol`: mean traffic volume per meter of major roadways
- `pct_rural`: percentage of county population living in rural area

---

### Back to bike crashes

Let's fit a model with `traffic_vol` and `pct_rural` for now:

```{r}
m1 <- glm(crashes ~ traffic_vol + pct_rural, 
          data = bike, family = "poisson")

round(summary(m1)$coef, 6)
```

.question[
What might we conclude / interpret from this model?
]

---

### Newton-Raphson (rough) implementation

Newton-Raphson update steps for Poisson regression: 

\begin{align*}
\boldsymbol\beta^{(t+1)} &= \boldsymbol\beta^{(t)} - \left(-\sum_{i = 1}^n e^{\mathbf{X}_i\boldsymbol\beta}\mathbf{X}_i\mathbf{X}_i^T \right)^{-1}\left(\sum_{i = 1}^n \left(y_i - e^{\mathbf{X}_i\boldsymbol\beta}\right)\mathbf{X}_i^T \right)
\end{align*}

Function for score vector, given vector `beta`, matrix `X`, and vector 
`y`:

```{r}
d1func <- function(beta, X, y){
  d1 <- rep(0, length(beta))
  for(i in 1:length(y)){
    d1 <- d1 + (y[i] - exp(X[i,] %*% beta)) %*% X[i,]
  }
  return(colSums(d1))
}
```

---

### Newton-Raphson (rough) implementation

Newton-Raphson update steps for Poisson regression: 

\begin{align*}
\boldsymbol\beta^{(t+1)} &= \boldsymbol\beta^{(t)} - \left(-\sum_{i = 1}^n e^{\mathbf{X}_i\boldsymbol\beta}\mathbf{X}_i\mathbf{X}_i^T \right)^{-1}\left(\sum_{i = 1}^n \left(y_i - e^{\mathbf{X}_i\boldsymbol\beta}\right)\mathbf{X}_i^T \right)
\end{align*}

Function for Hessian, given vector `beta`, matrix `X`, and vector 
`y`:

```{r}
d2func <- function(beta, X, y){
  d2 <- matrix(0, nrow = length(beta), ncol = length(beta))
  for(i in 1:length(y)){
    d2 <- d2 - t((exp(X[i,] %*% beta)) %*% X[i,]) %*% (X[i,])
  }
  return(d2)
}
```

---

### Newton-Raphson (rough) implementation

Some bookkeeping: 

```{r}
beta <- c(mean(log(bike$crashes)), 0, 0)
X <- cbind(1, bike$traffic_vol, bike$pct_rural)
y <- bike$crashes
iter <- 1
delta <- 1

temp <- matrix(0, nrow = 500, ncol = 3)
```

---

### Newton-Raphson (rough) implementation

Actual Newton-Raphson implementation:

\begin{align*}
\boldsymbol\beta^{(t+1)} &= \boldsymbol\beta^{(t)} - \left(-\sum_{i = 1}^n e^{\mathbf{X}_i\boldsymbol\beta}\mathbf{X}_i\mathbf{X}_i^T \right)^{-1}\left(\sum_{i = 1}^n \left(y_i - e^{\mathbf{X}_i\boldsymbol\beta}\right)\mathbf{X}_i^T \right)
\end{align*}

```{r}
while(delta > 0.000001 & iter < 500){
  old <- beta
  beta <- old - solve(d2func(beta = beta, X = X, y = y)) %*% 
                d1func(beta = beta, X = X, y = y)
  temp[iter,] <- beta
  
  delta <- sqrt(sum((beta - old)^2))
  iter <- iter + 1
}
```

---

### Newton-Raphson (rough) implementation

```{r}
iter
delta
beta

m1$coefficients
```

---

### Back to bike crashes

```{r, echo = F}
library(sf)
nc <- st_read(system.file("shape/nc.shp", package = "sf"), quiet = TRUE)
names(nc)[5] <- "county"
nc <- merge(nc, bike, by = "county")
par(mfrow = c(2, 1))
ggplot(data = nc) +
  geom_sf(aes(fill = pop/1000)) +
  scale_fill_gradient(low = "#fee8c8", high ="#7f0000") +
  theme_void() + 
  theme(legend.position = "bottom",
        plot.title = element_text(hjust = 0.5)) + 
  labs(fill = "",
       title = "County population (in thousands)")
ggplot(data = nc) +
  geom_sf(aes(fill = crashes)) +
  scale_fill_gradient(low = "#fee8c8", high ="#7f0000") +
  theme_void() + 
  theme(legend.position = "bottom",
        plot.title = element_text(hjust = 0.5)) + 
  labs(fill = "",
       title = "Number of bike crashes")
```

---

### Back to bike crashes

\begin{align*}
\log\left(E(Y | \mathbf{X})\right) &= \beta_0 + \beta_1(pop) + \beta_2(traffic) + \beta_3(rural)\\
\end{align*}
```{r}
m2 <- glm(crashes ~ traffic_vol + pct_rural + pop, 
          data = bike, family = "poisson")
```

\begin{align*}
\log\left( \frac{E(Y | \mathbf{X})}{pop} \right) &= \beta_0 + \beta_1(traffic) + \beta_2(rural)
\end{align*}
```{r}
m3 <- glm(crashes ~ traffic_vol + pct_rural, offset = log(pop),
          data = bike, family = "poisson")
```

.question[
What are the differences in the two models above?
]

---

### Back to bike crashes

\begin{align*}
\log\left( \frac{E(Y | \mathbf{X})}{pop} \right) &= \beta_0 + \beta_1(traffic) + \beta_2(rural) \\
\log\left(E(Y | \mathbf{X})\right) &=  \beta_0 + \beta_1(traffic) + \beta_2(rural) - \log(pop)
\end{align*}

Here, we are using `pop` as an .vocab[offset]. This means that our dependent 
variable is actually a *rate*, even though we are providing counts, and we can
look at covariate effects directly on this rate.

If we use `pop` as a covariate, then the response is no longer a rate of bike
crashes per unit population. However, we would be able to assess association
between population and number of bike crashes (conditionally on traffic volume
and urbanicity).

---

### Back to bike crashes

```{r}
round(summary(m1)$coef, 6)
round(summary(m2)$coef, 6)
round(summary(m3)$coef, 6)
```

---

### Poisson regression with offset

.question[
Can we simply use `bike$crashes/bike$pop` as our outcome variable in the
code we've already written?
]

```{r}
beta <- c(mean(log(bike$crashes)), 0, 0)
X <- cbind(1, bike$traffic_vol, bike$pct_rural)
y <- bike$crashes / bike$pop
iter <- 1
delta <- 1

temp <- matrix(0, nrow = 500, ncol = 3)

while(delta > 0.000001 & iter < 500){
  old <- beta
  beta <- old - solve(d2func(beta = beta, X = X, y = y)) %*% 
                d1func(beta = beta, X = X, y = y)
  temp[iter,] <- beta
  
  delta <- sqrt(sum((beta - old)^2))
  iter <- iter + 1
}
```

---

### Poisson regression with offset

```{r}
round(beta, 6)

round(m3$coefficients, 6)
```

.question[
They're close, but not quite right. Did something go wrong?
]

---

### Poisson regression with offset

```{r}
m3_wrong <- m2 <- glm(crashes/pop ~ traffic_vol + pct_rural, 
          data = bike, family = "poisson")

round(m3_wrong$coefficients, 6)

round(beta, 6)
```

.question[
What's happening? (keep in mind, all output on this page is **wrong**)
]

---

### Poisson regression with offset

Let's denote an offset term by $\omega$. If we directly use `crashes/pop` in 
our Poisson regression likelihood, we would have a log-likelihood along the 
lines of

\begin{align*}
\log \mathcal{L}&\propto \sum_{i = 1}^n \frac{y_i}{\omega_i}\mathbf{X}_i\boldsymbol\beta - e^{\mathbf{X}_i\boldsymbol\beta}
\end{align*}

This is incorrect. We cannot assume `crashes/pop` has a Poisson distribution.


---

### Poisson regression with offset

If we write the log-likelihood for a Poisson regression with offset correctly, 
we have:

\begin{align*}
\log\left(E(Y | \mathbf{X})\right) &= \beta_0 + \mathbf{X}^T\boldsymbol\beta - \log \boldsymbol\omega \\
\log \mathcal{L}&\propto \sum_{i = 1}^n y_i\mathbf{X}_i\boldsymbol\beta - \omega_i e^{\mathbf{X}_i\boldsymbol\beta}
\end{align*}

Thus, we must use this *correct* log-likelihood to determine the score vector
and Hessian for our Newton-Raphson implementation:

\begin{align*}
\nabla \log \mathcal{L}&= \sum_{i = 1}^n \left(y_i - \omega_ie^{\mathbf{X}_i\boldsymbol\beta}\right)\mathbf{X}_i^T\\
\nabla^2 \log \mathcal{L} &= -\sum_{i = 1}^n \omega_i e^{\mathbf{X}_i\boldsymbol\beta}\mathbf{X}_i\mathbf{X}_i^T
\end{align*}

---

### Poisson regression with offset

Newton-Raphson update steps for Poisson regression with offset: 

\begin{align*}
\boldsymbol\beta^{(t+1)} &= \boldsymbol\beta^{(t)} - \left(-\sum_{i = 1}^n \omega_ie^{\mathbf{X}_i\boldsymbol\beta}\mathbf{X}_i\mathbf{X}_i^T \right)^{-1}\left(\sum_{i = 1}^n \left(y_i - \omega_i e^{\mathbf{X}_i\boldsymbol\beta}\right)\mathbf{X}_i^T \right)
\end{align*}

In writing code, we must now specify the offset $\omega$ in addition to the
observed data $\mathbf{X}$, $y$, and candidate $\boldsymbol\beta$s.

---

### Poisson regression with offset

Functions for score vector and Hessian (including offset term):

```{r}
d1ofs <- function(beta, X, y, offset){
  d1 <- rep(0, length(beta))
  for(i in 1:length(y)){
    d1 <- d1 + (y[i] -  offset[i] *exp(X[i,] %*% beta)) %*% X[i,]
  }
  return(colSums(d1))
}

d2ofs <- function(beta, X, y, offset){
  d2 <- matrix(0, nrow = length(beta), ncol = length(beta))
  for(i in 1:length(y)){
    d2 <- d2 - offset[i] * t((exp(X[i,] %*% beta)) %*% X[i,]) %*% (X[i,])
  }
  return(d2)
}
```

---

### Poisson regression with offset

Implementing Newton-Raphson:

```{r}
beta <- c(mean(log(bike$crashes)), 0, 0)
X <- cbind(1, bike$traffic_vol, bike$pct_rural)
y <- bike$crashes
offset <- bike$pop
iter <- 1
delta <- 1

temp <- matrix(0, nrow = 500, ncol = 3)

while(delta > 0.000001 & iter < 500){
  old <- beta
  beta <- old - solve(d2ofs(beta = beta, X = X, y = y, offset = offset)) %*% 
                d1ofs(beta = beta, X = X, y = y, offset = offset)
  temp[iter,] <- beta
  
  delta <- sqrt(sum((beta - old)^2))
  iter <- iter + 1
}
```

---

### Poisson regression with offset

```{r}
round(beta, 6)
round(m3$coefficients, 6)
```

Our manual Newton-Raphson code lines up, as expected.

---

### Fisher Scoring

```{r, eval=TRUE, echo=FALSE, warning = F, message = F, out.width = "80%"}
include_graphics("img/fisher-scoring.png")
```

---

### Fisher Scoring

Fisher Scoring replaces $\nabla^2 \log \mathcal{L}$ with the expected 
Fisher information:

\begin{align*}
E\Large(\left(\nabla\log\mathcal{L} \right)\left(\nabla\log\mathcal{L} \right)^T \Large),
\end{align*}

which is asymptotically equivalent to the Hessian. It's often easier to 
implement because we don't need to take the second derivative.

Fisher Scoring is usually a bit more stable than vanilla Newton-Raphson and
the initial steps usually "get to where we want" faster. Newton-Raphson and 
Fisher Scoring updating steps are identical for GLMs under canonical link.